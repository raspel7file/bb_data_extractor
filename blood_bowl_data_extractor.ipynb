{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6261670-2a14-42a9-9ca8-f5c7840da2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8592e5-21da-453a-91e2-3c6d3ec4d161",
   "metadata": {},
   "source": [
    "This code extracts raw unprepared statistics for all matches from given competition and saves it into csv-file. You need Cyanide API key for this. To get the key contact Cyanide Studio Team com@cyanide-studio.com. I keep my private key in txt-file (it looks like https://web.cyanide-studio.com/ws/?bb=3&key=MYPRIVATEKEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0f474-a924-4cba-8dec-328a7389e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_key.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "api_key = str(lines[0]).split('=')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc676a-16ce-40c3-a2e3-e5b3d1cbc869",
   "metadata": {},
   "source": [
    "For extracting data you need competition_id (competition_name may be not unique). If you want to get matches from official ladder (like I do), you may execute following code:\n",
    "```python\n",
    "url = f'https://web.cyanide-studio.com/ws/bb3/competitions/?key={api_key}'\n",
    "\n",
    "response = re.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('Response: OK')\n",
    "    data = response.json()\n",
    "    if 'competitions' in data:\n",
    "        df = pd.json_normalize(data['competitions'])\n",
    "        for row in df[['id', 'name']].itertuples():\n",
    "            print(f\"{row.Index}: id={row.id}, name={row.name}\")\n",
    "    else:\n",
    "        print(\"No competitions in this league.\")\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "```\n",
    "and choose id.\n",
    "\n",
    "If you want to get matches from another league, you need its ID or name. Try something like this:\n",
    "```python\n",
    "league_name = 'Russian Blood Bowl League'\n",
    "url = f\"https://web.cyanide-studio.com/ws/bb3/competitions/?key={api_key}&league_name={league_name}\"\n",
    "\n",
    "response = re.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('Response: OK')\n",
    "    data = response.json()\n",
    "    if 'competitions' in data:\n",
    "        df = pd.json_normalize(data['competitions'])\n",
    "        for row in df[['id', 'name']].itertuples():\n",
    "            print(f\"{row.Index}: id={row.id}, name={row.name}\")\n",
    "    else:\n",
    "        print(\"No competitions in this league.\")\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7eb2c-3845-4b9d-a386-0ee2fc8315b7",
   "metadata": {},
   "source": [
    "Besides competition_id you have to define several parameters: \n",
    "- league_name: league your competition is from (default is Official League);\n",
    "- platform: doesn't matter much, used for naming saved csv files. I use \"pc\", \"playstation\", \"xbox\" or \"crossplay\";     \n",
    "- chunk_limit: defines amount of rows you'll get for 1 iteration, 1000 is max;  \n",
    "- request_limit: defines max amount of requests you code will make, current limits are 1000 per hour and 10000 per day, you have not to exceed these limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5b2dd-8dba-40dc-8360-bd15f463428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_id = '00000000-0000-0000-0000-000000000082'\n",
    "league_name = 'Official League'\n",
    "platform = 'crossplay'\n",
    "chunk_limit = 1000\n",
    "request_limit = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152067e-d1b9-4d4e-84a2-32980817f3ef",
   "metadata": {},
   "source": [
    "Execute following cell to get competition statistics. If something breaks mid-process, just execute cell again (if will save partially extracted data to temporary file and just continue from that point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c65139-d7e1-4189-ac58-3bac0585d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://web.cyanide-studio.com/ws/bb3/competitions/?key={api_key}&league_name={league_name}&limit=1000\"\n",
    "response = re.get(url)\n",
    "break_flag = 0 # If we need to break while cycle\n",
    "continue_flag = 0 # If we restart with previously saved data\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('Response: OK')\n",
    "    data = response.json()\n",
    "    \n",
    "    if 'competitions' in data:\n",
    "        df = pd.json_normalize(data['competitions'])\n",
    "        \n",
    "        if competition_id in df.id.values:\n",
    "            competition_start = df[df.id == competition_id].date_created.values[0]\n",
    "            competition_name = df[df.id == competition_id].name.values[0].replace(' ', '_')\n",
    "\n",
    "            if Path('temporary.csv').exists():\n",
    "                df = pd.read_csv('temporary.csv')\n",
    "                n = 0\n",
    "                print('Continue from previously loaded data.')\n",
    "                continue_flag = 1\n",
    "    \n",
    "            else:\n",
    "                print('Start extracting data.')\n",
    "                url = f'''https://web.cyanide-studio.com/ws/bb3/matches/?key={api_key}\\\n",
    "&competition_id={competition_id}\\\n",
    "&start={competition_start}\\\n",
    "&limit={chunk_limit}'''\n",
    "                n = 1\n",
    "                response = re.get(url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    print(f'{n} request: OK')\n",
    "                    data = response.json()\n",
    "                    df = pd.json_normalize(data['matches'])\n",
    "\n",
    "                else:\n",
    "                    print(f'{n} request: ERROR: {response.status_code}')\n",
    "                    df = pd.DataFrame()\n",
    "\n",
    "            try:\n",
    "                anchor_date = df.started.min()\n",
    "                combined_df = df.copy()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Something broke: {e}. May be, no matches in this competition.\")\n",
    "                break_flag = 1\n",
    "\n",
    "            try:\n",
    "                while (len(df) == chunk_limit and n < request_limit) or continue_flag:\n",
    "                    \n",
    "                    if len((df[df.started != anchor_date])) == 0 and not continue_flag:\n",
    "                        print(f\"Our chunks are too small. Or too many matches start at the same second. Can't proceed.\")\n",
    "                        print(f\"Make chunks larger, if possible.\")\n",
    "                        combined_df.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "                        combined_df.to_csv(f'temporary.csv', index=False)\n",
    "                        print(f\"Partially extracted data is saved to 'temporary.csv', you can continue later from this point.\")\n",
    "                        break_flag = 1\n",
    "                        break\n",
    "\n",
    "                    continue_flag = 0\n",
    "                    n += 1\n",
    "                    url = f'''https://web.cyanide-studio.com/ws/bb3/matches/?key={api_key}\\\n",
    "&competition_id={competition_id}\\\n",
    "&start={competition_start}\\\n",
    "&end={anchor_date}\n",
    "&limit={chunk_limit}'''\n",
    "                    response = re.get(url)\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        print(f'{n} request: OK')\n",
    "                        \n",
    "                    else:\n",
    "                        print(f'{n} request: ERROR: {response.status_code}')\n",
    "                    \n",
    "                    data = response.json()\n",
    "                    df = pd.json_normalize(data['matches'])\n",
    "                    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "                    anchor_date = combined_df.started.min()\n",
    "\n",
    "                if not break_flag:\n",
    "                    print(f\"Data extracted with {n} requests to 'matches' endpoint.\")\n",
    "                \n",
    "                    if n >= request_limit:\n",
    "                        print('Request limit exceeded. Try to continue later.')\n",
    "                        combined_df.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "                        combined_df.to_csv(f'temporary.csv', index=False)\n",
    "                        print(f\"Partially extracted data is saved to 'temporary.csv'\")\n",
    "                \n",
    "                    else:\n",
    "                        combined_df.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "                        combined_df.drop('api_match', axis=1, inplace=True)\n",
    "                        combined_df.info()\n",
    "                        combined_df.to_csv(f'{competition_name}_{platform}_raw_data.csv', index=False)\n",
    "                        print(f\"Data is saved to '{competition_name}_{platform}_raw_data.csv'\")\n",
    "                        \n",
    "                        if Path('temporary.csv').exists():\n",
    "                            Path('temporary.csv').unlink()\n",
    "                            print(f'temporary.csv deleted.')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Something broke: {e}. Data partially extracted with {n} requests to 'matches' endpoint.\")\n",
    "                combined_df.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "                combined_df.to_csv(f'temporary.csv', index=False)\n",
    "                print(f\"Partially extracted data is saved to 'temporary.csv', you can continue later from this point.\")\n",
    "        \n",
    "        else:\n",
    "            print(\"No competition with this ID in this league.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No competitions in this league.\")\n",
    "        \n",
    "else:\n",
    "    print(f'Error: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2df1e-e9bd-499c-8cbf-6ae7a2e67445",
   "metadata": {},
   "source": [
    "This way we get raw data \"as-is\", with some cells containing dictionaries. Working with this data format may be inconvenient, so following code will process data to \"expand\" those sells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d807ccd-36ef-459a-a072-9ad90e915c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dict_column(df, column_name):\n",
    "    \"\"\"\n",
    "    Function for \"flattening\" columns with dictionaries in cells.\n",
    "    \n",
    "    :param df: original DataFrame\n",
    "    :param column_name: column for \"expanding\"\n",
    "    :return: new DataFrame with expanded columns\n",
    "    \"\"\"\n",
    "    # Testing if column exists\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")\n",
    "    \n",
    "    # Get keys\n",
    "    keys = df[column_name].iloc[0][0].keys()\n",
    "    \n",
    "    # Make new columns out of keys\n",
    "    for i in range(len(df[column_name].iloc[0])):\n",
    "        for key in keys:\n",
    "            new_column_name = f\"{key}_{i + 1}\"\n",
    "            df[new_column_name] = df[column_name].apply(lambda x: x[i][key])\n",
    "    \n",
    "    # Delete original columns (if not needed)\n",
    "    df = df.drop(columns=[column_name])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd81619-38c6-4cd1-8647-b4f7d2a84eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(f'{competition_name}_{platform}_raw_data.csv').exists():\n",
    "\n",
    "    filename = f'{competition_name}_{platform}_raw_data.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    df['coaches'] = df['coaches'].apply(ast.literal_eval)\n",
    "    df['teams'] = df['teams'].apply(ast.literal_eval)\n",
    "\n",
    "    df = expand_dict_column(df, 'coaches')\n",
    "    df = expand_dict_column(df, 'teams')\n",
    "\n",
    "    df.to_csv(f'{competition_name}_{platform}_processed_data.csv', index=False)\n",
    "    print(f\"File {competition_name}_{platform}_processed_data.csv processed and saved.\")\n",
    "\n",
    "else:\n",
    "    print(\"File not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540457c-7d40-4e60-b7b9-825d10d24390",
   "metadata": {},
   "source": [
    "Result is 2 files: {competition_name}\\_{platform}\\_raw_data.csv with raw data and {competition_name}\\_{platform}\\_processed_data.csv with processed (more convenient to work with) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cfba1-4ce9-4b10-9f02-6d697bbb1050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
