{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6261670-2a14-42a9-9ca8-f5c7840da2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8592e5-21da-453a-91e2-3c6d3ec4d161",
   "metadata": {},
   "source": [
    "This code extracts raw unprepared statistics for all matches from given competition and saves it into csv-file. You need Cyanide API key for this. To get the key contact Cyanide Studio Team com@cyanide-studio.com. I keep my private key in txt-file (it looks like https://web.cyanide-studio.com/ws/?bb=3&key=MYPRIVATEKEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb0f474-a924-4cba-8dec-328a7389e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_key.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "api_key = str(lines[0]).split('=')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc676a-16ce-40c3-a2e3-e5b3d1cbc869",
   "metadata": {},
   "source": [
    "For extracting data you need competition_id (competition_name may be not unique). If you want to get matches from official ladder (like I do), you may execute following code:\n",
    "```python\n",
    "url = f'https://web.cyanide-studio.com/ws/bb3/competitions/?key={api_key}'\n",
    "\n",
    "response = re.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('Response: OK')\n",
    "    data = response.json()\n",
    "    if 'competitions' in data:\n",
    "        df = pd.json_normalize(data['competitions'])\n",
    "        for row in df[['id', 'name']].itertuples():\n",
    "            print(f\"{row.Index}: id={row.id}, name={row.name}\")\n",
    "    else:\n",
    "        print(\"No competitions in this league.\")\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "```\n",
    "and choose id.\n",
    "\n",
    "If you want to get matches from another league, you need its ID or name. Try something like this:\n",
    "```python\n",
    "league_name = 'Russian Blood Bowl League'\n",
    "url = f\"https://web.cyanide-studio.com/ws/bb3/competitions/?key={api_key}&league_name={league_name}\"\n",
    "\n",
    "response = re.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('Response: OK')\n",
    "    data = response.json()\n",
    "    if 'competitions' in data:\n",
    "        df = pd.json_normalize(data['competitions'])\n",
    "        for row in df[['id', 'name']].itertuples():\n",
    "            print(f\"{row.Index}: id={row.id}, name={row.name}\")\n",
    "    else:\n",
    "        print(\"No competitions in this league.\")\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7eb2c-3845-4b9d-a386-0ee2fc8315b7",
   "metadata": {},
   "source": [
    "Besides competition_id you have to define several parameters: \n",
    "- league_name: league your competition is from (default is Official League);\n",
    "- platform: doesn't matter much, used for naming saved csv files. I use \"pc\", \"playstation\", \"xbox\" or \"crossplay\";     \n",
    "- chunk_limit: defines amount of rows you'll get for 1 iteration, 1000 is max;  \n",
    "- request_limit: defines max amount of requests you code will make, current limits are 1000 per hour and 10000 per day, you have not to exceed these limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dce5b2dd-8dba-40dc-8360-bd15f463428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_id = '00000000-0000-0000-0000-000000000082'\n",
    "league_name = 'Official League'\n",
    "platform = 'crossplay'\n",
    "chunk_limit = 1000\n",
    "request_limit = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152067e-d1b9-4d4e-84a2-32980817f3ef",
   "metadata": {},
   "source": [
    "Execute following cell to get competition statistics. If something breaks mid-process, just execute cell again (if will save partially extracted data to temporary file and just continue from that point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23c65139-d7e1-4189-ac58-3bac0585d6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: OK\n",
      "Start extracting data.\n",
      "1 request: OK\n",
      "2 request: OK\n",
      "3 request: OK\n",
      "4 request: OK\n",
      "5 request: OK\n",
      "6 request: OK\n",
      "7 request: OK\n",
      "8 request: OK\n",
      "9 request: OK\n",
      "10 request: OK\n",
      "11 request: OK\n",
      "12 request: OK\n",
      "13 request: OK\n",
      "14 request: OK\n",
      "15 request: OK\n",
      "16 request: OK\n",
      "17 request: OK\n",
      "18 request: OK\n",
      "19 request: OK\n",
      "20 request: OK\n",
      "21 request: OK\n",
      "22 request: OK\n",
      "23 request: OK\n",
      "24 request: OK\n",
      "25 request: OK\n",
      "26 request: OK\n",
      "27 request: OK\n",
      "28 request: OK\n",
      "29 request: OK\n",
      "30 request: OK\n",
      "31 request: OK\n",
      "32 request: OK\n",
      "33 request: OK\n",
      "34 request: OK\n",
      "35 request: OK\n",
      "36 request: OK\n",
      "37 request: OK\n",
      "38 request: OK\n",
      "39 request: OK\n",
      "40 request: OK\n",
      "41 request: OK\n",
      "42 request: OK\n",
      "43 request: OK\n",
      "44 request: OK\n",
      "45 request: OK\n",
      "46 request: OK\n",
      "47 request: OK\n",
      "48 request: OK\n",
      "49 request: OK\n",
      "50 request: OK\n",
      "51 request: OK\n",
      "52 request: OK\n",
      "53 request: OK\n",
      "54 request: OK\n",
      "55 request: OK\n",
      "56 request: OK\n",
      "57 request: OK\n",
      "58 request: OK\n",
      "59 request: OK\n",
      "60 request: OK\n",
      "61 request: OK\n",
      "62 request: OK\n",
      "63 request: OK\n",
      "64 request: OK\n",
      "65 request: OK\n",
      "66 request: OK\n",
      "67 request: OK\n",
      "68 request: OK\n",
      "69 request: OK\n",
      "70 request: OK\n",
      "71 request: OK\n",
      "72 request: OK\n",
      "73 request: OK\n",
      "74 request: OK\n",
      "75 request: OK\n",
      "76 request: OK\n",
      "77 request: OK\n",
      "78 request: OK\n",
      "79 request: OK\n",
      "80 request: OK\n",
      "81 request: OK\n",
      "82 request: OK\n",
      "83 request: OK\n",
      "84 request: OK\n",
      "85 request: OK\n",
      "86 request: OK\n",
      "87 request: OK\n",
      "88 request: OK\n",
      "89 request: OK\n",
      "90 request: OK\n",
      "91 request: OK\n",
      "92 request: OK\n",
      "93 request: OK\n",
      "94 request: OK\n",
      "95 request: OK\n",
      "96 request: OK\n",
      "97 request: OK\n",
      "98 request: OK\n",
      "99 request: OK\n",
      "100 request: OK\n",
      "101 request: OK\n",
      "102 request: OK\n",
      "103 request: OK\n",
      "104 request: OK\n",
      "105 request: OK\n",
      "106 request: OK\n",
      "107 request: OK\n",
      "108 request: OK\n",
      "109 request: OK\n",
      "110 request: OK\n",
      "111 request: OK\n",
      "112 request: OK\n",
      "113 request: OK\n",
      "114 request: OK\n",
      "115 request: OK\n",
      "116 request: OK\n",
      "117 request: OK\n",
      "118 request: OK\n",
      "119 request: OK\n",
      "120 request: OK\n",
      "121 request: OK\n",
      "122 request: OK\n",
      "123 request: OK\n",
      "124 request: OK\n",
      "125 request: OK\n",
      "126 request: OK\n",
      "127 request: OK\n",
      "128 request: OK\n",
      "129 request: OK\n",
      "130 request: OK\n",
      "131 request: OK\n",
      "132 request: OK\n",
      "133 request: OK\n",
      "134 request: OK\n",
      "135 request: OK\n",
      "136 request: OK\n",
      "137 request: OK\n",
      "138 request: OK\n",
      "139 request: OK\n",
      "140 request: OK\n",
      "141 request: OK\n",
      "142 request: OK\n",
      "143 request: OK\n",
      "144 request: OK\n",
      "Data extracted with 144 requests to 'matches' endpoint.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 142961 entries, 0 to 143103\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   uuid             142961 non-null  object\n",
      " 1   id               142961 non-null  object\n",
      " 2   idcompetition    142961 non-null  object\n",
      " 3   competitionname  142961 non-null  object\n",
      " 4   started          142961 non-null  object\n",
      " 5   finished         142961 non-null  object\n",
      " 6   idleague         142961 non-null  object\n",
      " 7   leaguename       142961 non-null  object\n",
      " 8   stadium          8101 non-null    object\n",
      " 9   round            0 non-null       object\n",
      " 10  coaches          142961 non-null  object\n",
      " 11  teams            142961 non-null  object\n",
      "dtypes: object(12)\n",
      "memory usage: 14.2+ MB\n",
      "Data is saved to 'official_ladder_season_07_crossplay_raw_data.csv'\n"
     ]
    }
   ],
   "source": [
    "url = f\"https://web.cyanide-studio.com/ws/bb3/competitions/?key={api_key}&league_name={league_name}&limit=1000\"\n",
    "response = re.get(url)\n",
    "break_flag = 0 # If we need to break while cycle\n",
    "continue_flag = 0 # If we restart with previously saved data\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('Response: OK')\n",
    "    data = response.json()\n",
    "    \n",
    "    if 'competitions' in data:\n",
    "        df = pd.json_normalize(data['competitions'])\n",
    "        \n",
    "        if competition_id in df.id.values:\n",
    "            competition_start = df[df.id == competition_id].date_created.values[0]\n",
    "            competition_name = df[df.id == competition_id].name.values[0].replace(' ', '_')\n",
    "\n",
    "            if Path('temporary.csv').exists():\n",
    "                df = pd.read_csv('temporary.csv')\n",
    "                n = 0\n",
    "                print('Continue from previously loaded data.')\n",
    "                continue_flag = 1\n",
    "    \n",
    "            else:\n",
    "                print('Start extracting data.')\n",
    "                url = f'''https://web.cyanide-studio.com/ws/bb3/matches/?key={api_key}\\\n",
    "&competition_id={competition_id}\\\n",
    "&start={competition_start}\\\n",
    "&limit={chunk_limit}'''\n",
    "                n = 1\n",
    "                response = re.get(url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    print(f'{n} request: OK')\n",
    "                    data = response.json()\n",
    "                    df = pd.json_normalize(data['matches'])\n",
    "\n",
    "                else:\n",
    "                    print(f'{n} request: ERROR: {response.status_code}')\n",
    "                    df = pd.DataFrame()\n",
    "\n",
    "            try:\n",
    "                anchor_date = df.started.min()\n",
    "                combined_df = df.copy()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Something broke: {e}. May be, no matches in this competition.\")\n",
    "                break_flag = 1\n",
    "\n",
    "            try:\n",
    "                while (len(df) == chunk_limit and n < request_limit) or continue_flag:\n",
    "                    \n",
    "                    if len((df[df.started != anchor_date])) == 0 and not continue_flag:\n",
    "                        print(f\"Our chunks are too small. Or too many matches start at the same second. Can't proceed.\")\n",
    "                        print(f\"Make chunks larger, if possible.\")\n",
    "                        combined_df.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "                        combined_df.to_csv(f'temporary.csv', index=False)\n",
    "                        print(f\"Partially extracted data is saved to 'temporary.csv', you can continue later from this point.\")\n",
    "                        break_flag = 1\n",
    "                        break\n",
    "\n",
    "                    continue_flag = 0\n",
    "                    n += 1\n",
    "                    url = f'''https://web.cyanide-studio.com/ws/bb3/matches/?key={api_key}\\\n",
    "&competition_id={competition_id}\\\n",
    "&start={competition_start}\\\n",
    "&end={anchor_date}\n",
    "&limit={chunk_limit}'''\n",
    "                    response = re.get(url)\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        print(f'{n} request: OK')\n",
    "                        \n",
    "                    else:\n",
    "                        print(f'{n} request: ERROR: {response.status_code}')\n",
    "                    \n",
    "                    data = response.json()\n",
    "                    df = pd.json_normalize(data['matches'])\n",
    "                    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "                    anchor_date = combined_df.started.min()\n",
    "\n",
    "                if not break_flag:\n",
    "                    print(f\"Data extracted with {n} requests to 'matches' endpoint.\")\n",
    "                \n",
    "                    if n >= request_limit:\n",
    "                        print('Request limit exceeded. Try to continue later.')\n",
    "                        combined_df.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "                        combined_df.to_csv(f'temporary.csv', index=False)\n",
    "                        print(f\"Partially extracted data is saved to 'temporary.csv'\")\n",
    "                \n",
    "                    else:\n",
    "                        combined_df.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "                        combined_df.drop('api_match', axis=1, inplace=True)\n",
    "                        combined_df.info()\n",
    "                        combined_df.to_csv(f'{competition_name}_{platform}_raw_data.csv', index=False)\n",
    "                        print(f\"Data is saved to '{competition_name}_{platform}_raw_data.csv'\")\n",
    "                        \n",
    "                        if Path('temporary.csv').exists():\n",
    "                            Path('temporary.csv').unlink()\n",
    "                            print(f'temporary.csv deleted.')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Something broke: {e}. Data partially extracted with {n} requests to 'matches' endpoint.\")\n",
    "                combined_df.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "                combined_df.to_csv(f'temporary.csv', index=False)\n",
    "                print(f\"Partially extracted data is saved to 'temporary.csv', you can continue later from this point.\")\n",
    "        \n",
    "        else:\n",
    "            print(\"No competition with this ID in this league.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No competitions in this league.\")\n",
    "        \n",
    "else:\n",
    "    print(f'Error: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2df1e-e9bd-499c-8cbf-6ae7a2e67445",
   "metadata": {},
   "source": [
    "This way we get raw data \"as-is\", with some cells containing dictionaries. Working with this data format may be inconvenient, so following code will process data to \"expand\" those sells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d807ccd-36ef-459a-a072-9ad90e915c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dict_column(df, column_name):\n",
    "    \"\"\"\n",
    "    Function for \"flattening\" columns with dictionaries in cells.\n",
    "    \n",
    "    :param df: original DataFrame\n",
    "    :param column_name: column for \"expanding\"\n",
    "    :return: new DataFrame with expanded columns\n",
    "    \"\"\"\n",
    "    # Testing if column exists\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")\n",
    "    \n",
    "    # Get keys\n",
    "    keys = df[column_name].iloc[0][0].keys()\n",
    "    \n",
    "    # Make new columns out of keys\n",
    "    for i in range(len(df[column_name].iloc[0])):\n",
    "        for key in keys:\n",
    "            new_column_name = f\"{key}_{i + 1}\"\n",
    "            df[new_column_name] = df[column_name].apply(lambda x: x[i][key])\n",
    "    \n",
    "    # Delete original columns (if not needed)\n",
    "    df = df.drop(columns=[column_name])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfd81619-38c6-4cd1-8647-b4f7d2a84eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File official_ladder_season_07_crossplay_processed_data.csv processed and saved.\n"
     ]
    }
   ],
   "source": [
    "if Path(f'{competition_name}_{platform}_raw_data.csv').exists():\n",
    "\n",
    "    filename = f'{competition_name}_{platform}_raw_data.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    df['coaches'] = df['coaches'].apply(ast.literal_eval)\n",
    "    df['teams'] = df['teams'].apply(ast.literal_eval)\n",
    "\n",
    "    df = expand_dict_column(df, 'coaches')\n",
    "    df = expand_dict_column(df, 'teams')\n",
    "\n",
    "    df.to_csv(f'{competition_name}_{platform}_processed_data.csv', index=False)\n",
    "    print(f\"File {competition_name}_{platform}_processed_data.csv processed and saved.\")\n",
    "\n",
    "else:\n",
    "    print(\"File not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540457c-7d40-4e60-b7b9-825d10d24390",
   "metadata": {},
   "source": [
    "Result is 2 files: {competition_name}\\_{platform}\\_raw_data.csv with raw data and {competition_name}\\_{platform}\\_processed_data.csv with processed (more convenient to work with) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cfba1-4ce9-4b10-9f02-6d697bbb1050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
